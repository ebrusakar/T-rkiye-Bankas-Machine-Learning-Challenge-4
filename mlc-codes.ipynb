{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1d51db",
   "metadata": {},
   "source": [
    "# Türkiye İş Bankası Machine Learning Challenge #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0002d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing\n",
    "import numpy as np # working with arrays\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "from termcolor import colored as cl # text customization\n",
    "import itertools # advanced tools\n",
    "from sklearn.preprocessing import StandardScaler # data normalization\n",
    "from sklearn.model_selection import train_test_split # data split\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN algorithm\n",
    "from sklearn.linear_model import LogisticRegression # Logistic regression algorithm\n",
    "from sklearn.svm import SVC # SVM algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\n",
    "from xgboost import XGBClassifier # XGBoost algorithm\n",
    "from sklearn.metrics import confusion_matrix # evaluation metric\n",
    "from sklearn.metrics import accuracy_score # evaluation metric\n",
    "from sklearn.metrics import f1_score # evaluation metric\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from scipy.special import expit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import array\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import prince\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "real_test = pd.read_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610a0b9",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ae9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TXN_TIME=pd.cut(train.TXN_TIME,bins=[-1,600,1200,1800,2400],labels=['1','2','3','4'])  \n",
    "train[\"TXN_TIME\"] = train[\"TXN_TIME\"].astype('int')\n",
    "train.DAY_OF_MONTH=pd.cut(train.DAY_OF_MONTH,bins=[0,7,14,21],labels=['1','2','3'])\n",
    "train[\"DAY_OF_MONTH\"] = train[\"DAY_OF_MONTH\"].astype('int')\n",
    "train[\"DAY_OF_WEEK\"] = train[\"DAY_OF_WEEK\"].astype('int')\n",
    "train[\"TARGET\"] = train[\"TARGET\"].astype('int')\n",
    "train[\"TXN_AMNT\"] = train[\"TXN_AMNT\"].astype('int')\n",
    "del train['TXN_TRM']\n",
    "train.drop_duplicates(inplace=True, ignore_index=True)\n",
    "train.info()\n",
    "train.isna().sum() \n",
    "train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0489af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of TARGET vs. DAY_OF_WEEK\n",
    "cross_tab = pd.crosstab(index=train[\"TARGET\"], \n",
    "                           columns=train[\"DAY_OF_WEEK\"])\n",
    "\n",
    "cross_tab.index= [\"0\",\"1\"]\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical data\n",
    "categorical = ['CST_NR', 'CC_NR', 'TXN_SOURCE', 'TXN_ENTRY','CITY','COUNTRY','MC_NAME','MC_ID','MCC_CODE'] \n",
    "for feature in categorical:\n",
    "  train[feature] = train[feature].mask(train[feature].map(train[feature].value_counts(normalize=True)) < 0.001, 'Other')\n",
    "\n",
    "#import pandas as pd\n",
    "train = pd.get_dummies(train, columns = categorical)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to scale the data \n",
    "scaler = MinMaxScaler()\n",
    "data=train.drop(['TXN_AMNT', 'TARGET'], axis='columns')\n",
    "data_rescaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce859",
   "metadata": {},
   "source": [
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9004fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to select the number of components\n",
    "pca = PCA().fit(data_rescaled)\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Explained Variances')\n",
    "plt.show()  \n",
    "\n",
    "#PCA\n",
    "pca = prince.PCA(\n",
    "     n_components=110,\n",
    "     n_iter=3,\n",
    "     rescale_with_mean=True,\n",
    "     rescale_with_std=True,\n",
    "     copy=True,\n",
    "     check_input=True,\n",
    "     engine='auto',\n",
    "     random_state=42)\n",
    "pca = pca.fit(data_rescaled)\n",
    "pca.transform(data_rescaled).head() \n",
    "pca.eigenvalues_\n",
    "principalComponents = pca.fit_transform(data_rescaled)\n",
    "train_pc = pd.DataFrame(data = principalComponents)\n",
    "train_pc['TARGET'] = train['TARGET']\n",
    "train_pc['TXN_AMNT'] = train['TXN_AMNT']\n",
    "train_pc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9d698",
   "metadata": {},
   "source": [
    "Normalize ‘Amount’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ddf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pc['TXN_AMNT(Normalized)'] = StandardScaler().fit_transform(train_pc['TXN_AMNT'].values.reshape(-1,1))\n",
    "train_pc = train_pc.drop(columns = ['TXN_AMNT'], axis=1) # This column is not necessary anymore.\n",
    "train_pc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658ce87",
   "metadata": {},
   "source": [
    "Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120411d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_pc[\"TARGET\"]\n",
    "X = train_pc.drop(['TARGET'], axis='columns')\n",
    "#\n",
    "# Create training and test split\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "#\n",
    "# Create a pipeline; This will be passed as an estimator to learning curve method\n",
    "#\n",
    "pipeline = make_pipeline(StandardScaler(),\n",
    "                        LogisticRegression(penalty='l2', solver='lbfgs', random_state=1, max_iter=10000))\n",
    "#\n",
    "# Use learning curve to get training and test scores along with train sizes\n",
    "#\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipeline, X=X_train, y=y_train,\n",
    "                                                       cv=10, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                     n_jobs=1)\n",
    "#\n",
    "# Calculate training and test mean and std\n",
    "#\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "#\n",
    "# Plot the learning curve\n",
    "#\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e3525",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_pc.drop('TARGET', axis=1)\n",
    "y = train_pc['TARGET']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# We are transforming data to numpy array to implementing with keras\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELING\n",
    "\n",
    "# 1. Decision Tree\n",
    "print('Decision Tree')\n",
    "tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_yhat = tree_model.predict(X_test)\n",
    "\n",
    "# 3. Logistic Regression\n",
    "print('Logistic Regression')\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)\n",
    "\n",
    "\n",
    "# 5. Random Forest Tree\n",
    "print('Random Forest Tree')\n",
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)\n",
    "\n",
    "# 6. XGBoost\n",
    "print('XGBoost')\n",
    "xgb = XGBClassifier(max_depth = 4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy score\n",
    "\n",
    "print(cl('ACCURACY SCORE', attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold'], color = 'red'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48fe88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Artificial Neural Networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "model = Sequential([\n",
    "    Dense(units=20, input_dim = X_train.shape[1], activation='relu'),\n",
    "    Dense(units=24,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=20,activation='relu'),\n",
    "    Dense(units=24,activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=30, epochs=5)\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy: {:.2f}%\\nTest Loss: {}'.format(score[1]*100,score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92171a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = train_pc['TARGET'].value_counts()\n",
    "occ\n",
    "# Print the ratio of fraud cases\n",
    "ratio_cases = occ/len(train_pc.index)\n",
    "print(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}') # class imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e7554",
   "metadata": {},
   "source": [
    "# SMOTE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e301ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "X_smote, y_smote = SMOTE().fit_resample(X, y)\n",
    "X_smote = pd.DataFrame(X_smote)\n",
    "y_smote = pd.DataFrame(y_smote)\n",
    "y_smote.iloc[:,0].value_counts()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size = 30, epochs = 5)\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy: {:.2f}%\\nTest Loss: {}'.format(score[1]*100,score[0]))\n",
    "y_pred = model.predict(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred.round())\n",
    "sns.heatmap(cm, annot=True, fmt='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = y_smote.value_counts()\n",
    "occ\n",
    "# Print the ratio of fraud cases\n",
    "ratio_cases = occ/len(y_smote.index)\n",
    "print(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}') # problem solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e096aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "y_test2 = pd.DataFrame(y_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm2 = confusion_matrix(y_test2, y_pred2.round())\n",
    "sns.heatmap(cm2, annot=True, fmt='.0f', cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# plt\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test2, y_pred2.round())\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169af6c",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decision Tree\n",
    "print('Decision Tree')\n",
    "tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_yhat = tree_model.predict(X_test)\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print('Logistic Regression')\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)\n",
    "\n",
    "\n",
    "# 3. Random Forest Tree\n",
    "print('Random Forest Tree')\n",
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)\n",
    "\n",
    "# 4. XGBoost\n",
    "print('XGBoost')\n",
    "xgb = XGBClassifier(max_depth = 4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)\n",
    "\n",
    "# 5. ANN\n",
    "print('ANN')\n",
    "model.fit(X_train, y_train, batch_size = 30, epochs = 5)\n",
    "ann_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Blending model\n",
    "print('Blending model')\n",
    "blended= 0.1*y_pred2.round()+(0.5*knn_yhat+ 0.1*xgb_yhat +0.1*rf_yhat+0.1*lr_yhat+0.1*tree_yhat).reshape(320979,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8fefd",
   "metadata": {},
   "source": [
    " Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl('ACCURACY SCORE', attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the ANN model is {}'.format(accuracy_score(y_test, ann_pred.round())), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Blending model is {}'.format(accuracy_score(y_test, blended.round())), attrs = ['bold'], color = 'red'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fa359",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters of An Artificial Neural Network Leveraging Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "def build_model(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "  hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "  hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "  model.add(Dense(units=hp_units1, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2,1e-3,1e-4])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "      loss=msle,\n",
    "      metrics=[msle]\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "# HyperBand algorithm from keras tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mean_squared_logarithmic_error',\n",
    "    max_epochs=10,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='keras_tuner_demo'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "\n",
    "#Trial 31 Complete [00h 03m 08s]\n",
    "#val_mean_squared_logarithmic_error: 0.03584810346364975\n",
    "\n",
    "#Best val_mean_squared_logarithmic_error So Far: 0.024163799360394478\n",
    "#Total elapsed time: 00h 52m 05s\n",
    "#INFO:tensorflow:Oracle triggered exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h_param in [f\"units{i}\" for i in range(1,4)] + ['learning_rate']:\n",
    "  print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))\n",
    "\n",
    "#units1 192\n",
    "#units2 352\n",
    "#units3 352\n",
    "#learning_rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c85d9",
   "metadata": {},
   "source": [
    "# XGBoost Hyperparameter Tuning Using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a875869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Regressor Optuna\n",
    "def xgbobj(trial , data = X_smote , target = y_smote):\n",
    "        params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"subsample\": trial.suggest_discrete_uniform(\"subsample\",0.8,1,0.1),\n",
    "        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\",0.3,0.8,0.1),\n",
    "        \"eta\": 0.01,\n",
    "        \"reg_alpha\": trial.suggest_int(\"reg_alpha\",0,30,1),\n",
    "        \"reg_lambda\": trial.suggest_int(\"reg_lambda\",0,1,0.1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\",3,5,1),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",1,3,1),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, tree_method = \"gpu_hist\",random_state=42)\n",
    "    model.fit(X_train,y_train,eval_set = [(X_test,y_test)] ,early_stopping_rounds=100, verbose = False , eval_metric = \"auc\")\n",
    "\n",
    "    y_preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction = \"minimize\")\n",
    "study.optimize(xgbobj, n_trials = 15)\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial: score {}, params {}\".format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "\n",
    "#Number of finished trials: 15\n",
    "#Best trial: score 0.3579398863954667, params {'subsample': 0.8, 'colsample_bytree': 0.7, 'reg_alpha': 25, \n",
    "#'reg_lambda': 0, 'max_depth': 5, 'min_child_weight': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost with Optuna Hyperparameters\n",
    "modelxgb = xgb.XGBRegressor(n_estimators= 100, tree_method='gpu_hist', subsample= 0.8, colsample_bytree= 0.7,\n",
    "       eta= 0.01, reg_alpha= 25, reg_lambda= 0, max_depth= 5, min_child_weight= 2).fit(X_train , y_train)\n",
    "\n",
    "\n",
    "xgbpred = np.expm1(modelxgb.predict(X_test))\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# plt\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, xgbpred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e393f",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decision Tree\n",
    "print('Decision Tree')\n",
    "tree_model2 = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "tree_model2.fit(X_train, y_train)\n",
    "tree_yhat2 = tree_model2.predict(X_test)\n",
    "\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print('Logistic Regression')\n",
    "lr2 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr2.fit(X_train, y_train)\n",
    "lr_yhat2 = lr2.predict(X_test)\n",
    "\n",
    "\n",
    "# 3. Random Forest Tree\n",
    "print('Random Forest Tree')\n",
    "rf2 = RandomForestClassifier(max_depth = 4)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf_yhat2 = rf2.predict(X_test)\n",
    "\n",
    "# 4. XGBoost with Optuna Hyperparameters\n",
    "print('XGBoost')\n",
    "xgbpred = np.expm1(modelxgb.predict(X_test))\n",
    "\n",
    "\n",
    "# 5.Artificial Neural Networks with Keras Tuner\n",
    "ann_pred = tf.keras.activations.sigmoid(best_model.predict(X_test))\n",
    "ann_pred = np.where(pd.DataFrame(ann_pred.numpy())>0.5, 1, 0) \n",
    "\n",
    "\n",
    "# 6.Blending Model\n",
    "blended=0.1*ann_pred+(0.15*xgb_yhat2+0.25*rf_yhat2+0.25*lr_yhat2+0.25*tree_yhat2).reshape(320979,1)\n",
    "blended=np.where(pd.DataFrame(blended)>0.5, 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8a94a",
   "metadata": {},
   "source": [
    "Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0eaf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl('ACCURACY SCORE', attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat2)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat2)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat2)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the ANN model is {}'.format(accuracy_score(y_test, ann_pred)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the XGB_tuned model is {}'.format(accuracy_score(y_test, xgbpred)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Blended model is {}'.format(accuracy_score(y_test, blended)), attrs = ['bold'], color = 'red'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4322e2f",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.read_csv(\"test.csv\")\n",
    "real_test.TXN_TIME=pd.cut(real_test.TXN_TIME,bins=[-1,600,1200,1800,2400],labels=['1','2','3','4'])  \n",
    "real_test[\"TXN_TIME\"] = real_test[\"TXN_TIME\"].astype('int')\n",
    "real_test.DAY_OF_MONTH=pd.cut(real_test.DAY_OF_MONTH,bins=[20,30],labels=['4'])\n",
    "real_test[\"DAY_OF_MONTH\"] = real_test[\"DAY_OF_MONTH\"].astype('int')\n",
    "real_test[\"DAY_OF_WEEK\"] = real_test[\"DAY_OF_WEEK\"].astype('int')\n",
    "real_test[\"TXN_AMNT\"] = real_test[\"TXN_AMNT\"].astype('int')\n",
    "del real_test['TXN_TRM']\n",
    "del real_test['ID']\n",
    "real_test.info()\n",
    "real_test.isna().sum() \n",
    "real_test.describe(include='all')\n",
    "#categorical data\n",
    "categorical = ['CST_NR', 'CC_NR', 'TXN_SOURCE', 'TXN_ENTRY','CITY','COUNTRY','MC_NAME','MC_ID','MCC_CODE'] \n",
    "for feature in categorical:\n",
    "  real_test[feature] = real_test[feature].mask(real_test[feature].map(real_test[feature].value_counts(normalize=True)) < 0.001,\n",
    "                                               'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.get_dummies(real_test, columns = categorical)\n",
    "real_test.head()\n",
    "\n",
    "\n",
    "real_test_data=real_test.drop('TXN_AMNT', axis= 1)\n",
    "real_test_rescaled = scaler.fit_transform(real_test_data)\n",
    "\n",
    "#PCA\n",
    "pca = pca.fit(real_test_rescaled)\n",
    "pca.transform(real_test_rescaled).head() \n",
    "pca.eigenvalues_\n",
    "principalComponents2 = pca.fit_transform(real_test_rescaled)\n",
    "real_test_pc = pd.DataFrame(data = principalComponents2)\n",
    "real_test_pc['TXN_AMNT'] = real_test['TXN_AMNT']\n",
    "real_test_pc.head()\n",
    "\n",
    "\n",
    "#Normalize ‘Amount’\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "real_test_pc['TXN_AMNT(Normalized)'] = StandardScaler().fit_transform(real_test_pc['TXN_AMNT'].values.reshape(-1,1))\n",
    "real_test_pc = real_test_pc.drop(columns = ['TXN_AMNT'], axis=1) # This column is not necessary anymore.\n",
    "real_test_pc.head()\n",
    "\n",
    "#Data PreProcessing\n",
    "X_train = train_pc.drop('TARGET', axis=1)\n",
    "y_train = train_pc['TARGET']\n",
    "X_test = real_test_pc\n",
    "\n",
    "#SMOTE Sampling\n",
    "X_train_smote, y_train_smote = SMOTE().fit_resample(X_train, y_train)\n",
    "X_train_smote = pd.DataFrame(X_train_smote)\n",
    "y_train_smote = pd.DataFrame(y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79633861",
   "metadata": {},
   "source": [
    " Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Decision Tree\n",
    "print('Decision Tree')\n",
    "tree_model3 = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "tree_model3.fit(X_train_smote, y_train_smote)\n",
    "tree_yhat3 = tree_model3.predict(X_test)\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print('Logistic Regression')\n",
    "lr3 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr3.fit(X_train_smote, y_train_smote)\n",
    "lr_yhat3 = lr3.predict(X_test)\n",
    "\n",
    "\n",
    "# 3. Random Forest Tree\n",
    "print('Random Forest Tree')\n",
    "rf3 = RandomForestClassifier(max_depth = 4)\n",
    "rf3.fit(X_train_smote, y_train_smote)\n",
    "rf_yhat3 = rf3.predict(X_test)\n",
    "\n",
    "# 4. XGBoost with Optuna Hyperparameters\n",
    "print('XGBoost')\n",
    "modelxgb2 = xgb.XGBRegressor(n_estimators= 100, tree_method='gpu_hist', subsample= 0.8, colsample_bytree= 0.7,\n",
    "       eta= 0.01, reg_alpha= 25, reg_lambda= 0, max_depth= 5, min_child_weight= 2).fit(X_train_smote,y_train_smote)\n",
    "xgbpred2 = np.expm1(modelxgb2.predict(X_test))\n",
    "\n",
    "\n",
    "# 5.Artificial Neural Networks with Keras Tuner\n",
    "print('Artificial Neural Networks')\n",
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.fit(\n",
    "    X_train_smote, \n",
    "    y_train_smote,\n",
    "    epochs=10,\n",
    "    batch_size=30\n",
    ")\n",
    "ann_pred2 = tf.keras.activations.sigmoid(best_model.predict(X_test))\n",
    "\n",
    "\n",
    "# 6.Blending Model\n",
    "print('Blending Model')\n",
    "blended2= 0.1*ann_pred2+(0.15*xgbpred2+0.25*rf_yhat3+0.25*lr_yhat3+0.25*tree_yhat3).reshape(326232,1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb30f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"sample_submission.csv\")\n",
    "submit.iloc[:,1]=pd.DataFrame(blended2)\n",
    "submit.to_csv(\"late3_submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
